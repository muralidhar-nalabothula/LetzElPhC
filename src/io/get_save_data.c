/**
 * @file
 * @brief Read lattice, pseudopotential, and wavefunction data from SAVE
 * directory
 *
 * This file implements I/O routines to read DFT data (lattice parameters,
 * atomic positions, symmetries, wavefunctions, pseudopotentials) from the
 * SAVE directory generated by DFT codes (currently supports Quantum ESPRESSO
 * via YAMBO format).
 */

#include <math.h>
#include <mpi.h>
#include <netcdf.h>
#include <netcdf_par.h>
#include <stdbool.h>
#include <stddef.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

#include "common/ELPH_timers.h"
#include "common/constants.h"
#include "common/cwalk/cwalk.h"
#include "common/dtypes.h"
#include "common/error.h"
#include "common/numerical_func.h"
#include "common/parallel.h"
#include "dvloc/dvloc.h"
#include "elphC.h"
#include "io.h"
#include "io/qe/qe_io.h"
#include "mpi_bcast.h"
#include "nonloc/fcoeff.h"
#include "symmetries/symmetries.h"

/** @brief Serial NetCDF read of entire variable */
static void quick_read(const int ncid, char* var_name, void* data_out);

/** @brief Allocate and set G-vectors for a specific k-point */
static void alloc_and_set_Gvec(
    ELPH_float* gvec, const ND_int ik, const ELPH_float* totalGvecs,
    const ND_int ng_total, const ELPH_float* Gvecidxs, const ND_int ng_shell,
    const ELPH_float* lat_param, const ND_int nG, const ND_int nG_shift);

/** @brief Parallel NetCDF read of variable slice */
static void quick_read_sub(const int ncid, char* var_name, const size_t* startp,
                           const size_t* countp, void* data_out);

/** @brief Read wavefunction from SAVE directory for specific k-point and spin
 */
static void get_wfc_from_save(ND_int spin_stride_len, ND_int ik, ND_int nkiBZ,
                              ND_int nspin, ND_int nspinor, ND_int start_band,
                              ND_int nbnds, ND_int nG, ND_int G_shift,
                              const char* save_dir, char* work_array,
                              const size_t work_array_len, ELPH_cmplx* out_wfc,
                              MPI_Comm comm);

/** @brief Determine which MPI pool should perform I/O for given k-point */
static inline ND_int get_wf_io_pool(ND_int ik, ND_int q, ND_int r);

/** @brief Check if element exists in array */
static bool check_ele_in_array(ND_int* arr_in, ND_int nelements,
                               ND_int check_ele);

/**
 * @brief Read and allocate all DFT data from SAVE directory
 *
 * Reads lattice parameters, symmetries, atomic positions, pseudopotentials,
 * wavefunctions, and related data from the SAVE directory generated by
 * DFT/DFPT calculations. Data is read primarily by rank 0 and broadcast
 * to other MPI processes. Wavefunctions are distributed across k-point pools.
 *
 * @param[in] SAVEdir Path to SAVE directory containing DFT output files
 * @param[in] Comm MPI communicator structure for parallel I/O
 * @param[in] start_band Starting band index (Fortran indexing: 1-based)
 * @param[in] end_band Ending band index (Fortran indexing: 1-based, inclusive)
 * @param[out] wfcs Allocated array of wavefunctions in iBZ, shape: (nkpts_iBZ)
 * @param[in] ph_save_dir Path to phonon SAVE directory for pseudopotential
 * files
 * @param[out] lattice Lattice structure containing k-points, symmetries, atoms
 * @param[out] pseudo Pseudopotential structure with UPF data and projectors
 * @param[out] phonon Phonon structure (q-points populated from ph_save_dir)
 * @param[in] dft_code DFT code identifier (currently only DFT_CODE_QE
 * supported)
 *
 * @note start_band and end_band use Fortran convention (1-based indexing)
 * @note Invalid band ranges default to computing all bands [1, total_bands]
 * @note Wavefunctions are distributed by k-point pools for parallel efficiency
 * @note ph_save_dir must be accessible on all MPI processes
 * @note Only UPF2 pseudopotentials are currently supported
 */
void read_and_alloc_save_data(char* SAVEdir, const struct ELPH_MPI_Comms* Comm,
                              ND_int start_band, ND_int end_band,
                              struct WFC** wfcs, char* ph_save_dir,
                              struct Lattice* lattice, struct Pseudo* pseudo,
                              struct Phonon* phonon,
                              enum ELPH_dft_code dft_code)
{
    ELPH_start_clock("Save I.O");

    int mpi_error;

    char** pseudo_pots = NULL;

    char* pp_head = "ns.kb_pp_pwscf";
    if (dft_code == DFT_CODE_QE)
    {
        get_data_from_qe(lattice, phonon, ph_save_dir, &pseudo_pots, Comm);
    }
    else
    {
        error_msg("Only QE supported");
    }

    lattice->nfftz_loc = get_mpi_local_size_idx(
        lattice->fft_dims[2], &(lattice->nfftz_loc_shift), Comm->commK);

    if (lattice->nfftz_loc < 1)
    {
        error_msg(
            "Some cpus do not contain plane waves. Over parallelization !.");
    }

    int dbid, ppid, tempid, retval;

    size_t temp_str_len = strlen(ph_save_dir) + strlen(SAVEdir) + 100;
    char* temp_str = malloc(temp_str_len);
    CHECK_ALLOC(temp_str);

    int nkBZ;

    char* elements = malloc(3 * 104);
    CHECK_ALLOC(elements);
    {
        char* temp =
            "NA\0H \0He\0Li\0Be\0B \0C \0N \0O "
            "\0F \0Ne\0Na\0Mg\0Al\0Si\0P \0S \0Cl\0Ar\0K \0Ca"
            "\0Sc\0Ti\0V \0Cr\0Mn\0Fe\0Co\0Ni\0Cu\0Zn\0Ga\0Ge"
            "\0As\0Se\0Br\0Kr\0Rb\0Sr\0Y \0Zr\0Nb\0Mo\0Tc\0Ru"
            "\0Rh\0Pd\0Ag\0Cd\0In\0Sn\0Sb\0Te\0I \0Xe\0Cs\0Ba"
            "\0La\0Ce\0Pr\0Nd\0Pm\0Sm\0Eu\0Gd\0Tb\0Dy\0Ho\0Er"
            "\0Tm\0Yb\0Lu\0Hf\0Ta\0W \0Re\0Os\0Ir\0Pt\0Au\0Hg"
            "\0Tl\0Pb\0Bi\0Po\0At\0Rn\0Fr\0Ra\0Ac\0Th\0Pa\0U "
            "\0Np\0Pu\0Am\0Cm\0Bk\0Cf\0Es\0Fm\0Md\0No\0Lr\0";
        memcpy(elements, temp, 3 * 104);
    }

    if (Comm->commW_rank == 0)
    {
        cwk_path_join(SAVEdir, "ndb.kindx", temp_str, temp_str_len);

        if ((retval = nc_open(temp_str, NC_NOWRITE, &tempid)))
        {
            ERR(retval);
        }

#if defined(YAMBO_LT_5_1)
        ELPH_float kindx_pars[7];
        quick_read(tempid, "PARS", kindx_pars);
        nkBZ = (int)rint(kindx_pars[0]);
#else
        int nkBZ_read;
        quick_read(tempid, "nXkbz", &nkBZ_read);
        nkBZ = nkBZ_read;
#endif

        if ((retval = nc_close(tempid)))
        {
            ERR(retval);
        }
    }

    mpi_error = MPI_Bcast(&nkBZ, 1, MPI_INT, 0, Comm->commW);
    MPI_error_msg(mpi_error);

    if (nkBZ / Comm->nkpools < 1)
    {
        error_msg(
            "There are no kpoints in some cpus, Make sure nkpool < # of "
            "kpoints in full BZ.");
    }

    lattice->nkpts_BZ = nkBZ;

    ELPH_float dimensions[18];
    if (Comm->commW_rank == 0)
    {
        cwk_path_join(SAVEdir, "ns.db1", temp_str, temp_str_len);
        if ((retval = nc_open(temp_str, NC_NOWRITE, &dbid)))
        {
            ERR(retval);
        }
        quick_read(dbid, "DIMENSIONS", dimensions);
    }

    mpi_error = MPI_Bcast(dimensions, 18, ELPH_MPI_float, 0, Comm->commW);
    MPI_error_msg(mpi_error);

    lattice->nspinor = rint(dimensions[11]);
    lattice->nspin = rint(dimensions[12]);
    lattice->timerev = rint(dimensions[9]);
    lattice->total_bands = rint(dimensions[5]);
    lattice->nsym = rint(dimensions[10]);
    lattice->nkpts_iBZ = rint(dimensions[6]);

    int nibz = lattice->nkpts_iBZ;

    if (start_band < 1 || end_band < 1)
    {
        if (Comm->commW_rank == 0)
        {
            fprintf(stderr,
                    "Warning : invalid bands used in calculation. computing "
                    "matrix elements for all bands, "
                    "Bands index belong to [1,nbnds] \n");
        }
        start_band = 1;
        end_band = lattice->total_bands;
    }
    if (start_band > lattice->total_bands || end_band > lattice->total_bands ||
        start_band >= end_band)
    {
        if (Comm->commW_rank == 0)
        {
            fprintf(
                stderr,
                "Warning : invalid bands used in calculation. computing matrix "
                "elements for all bands \n");
        }
        start_band = 1;
        end_band = lattice->total_bands;
    }

    lattice->start_band = start_band;
    lattice->end_band = end_band;
    lattice->nbnds = end_band - start_band + 1;

    ELPH_float lat_param[3];
    if (Comm->commW_rank == 0)
    {
        quick_read(dbid, "LATTICE_PARAMETER", lat_param);
    }

    mpi_error = MPI_Bcast(lat_param, 3, ELPH_MPI_float, 0, Comm->commW);
    MPI_error_msg(mpi_error);

    lattice->kpt_iredBZ = malloc(sizeof(ELPH_float) * 3 * nibz);
    CHECK_ALLOC(lattice->kpt_iredBZ);

    lattice->syms = malloc(sizeof(struct symmetry) * lattice->nsym);
    CHECK_ALLOC(lattice->syms);

    if (Comm->commW_rank == 0)
    {
        ELPH_float* sym_temp = malloc(sizeof(ELPH_float) * 12 * lattice->nsym);
        CHECK_ALLOC(sym_temp);

        ELPH_float* tau = sym_temp + 9 * lattice->nsym;

        ELPH_float* kiBZtmp = malloc(sizeof(ELPH_float) * 3 * nibz);
        CHECK_ALLOC(kiBZtmp);

        quick_read(dbid, "LATTICE_VECTORS", lattice->alat_vec);
        quick_read(dbid, "SYMMETRY", sym_temp);
        quick_read(dbid, "K-POINTS", kiBZtmp);

        for (ND_int i = 0; i < (3 * lattice->nsym); ++i)
        {
            tau[i] = 0;
        }

        for (ND_int i = 0; i < nibz; ++i)
        {
            kiBZtmp[i + 0 * nibz] /= lat_param[0];
            kiBZtmp[i + 1 * nibz] /= lat_param[1];
            kiBZtmp[i + 2 * nibz] /= lat_param[2];

            lattice->kpt_iredBZ[0 + i * 3] = kiBZtmp[i + 0 * nibz];
            lattice->kpt_iredBZ[1 + i * 3] = kiBZtmp[i + 1 * nibz];
            lattice->kpt_iredBZ[2 + i * 3] = kiBZtmp[i + 2 * nibz];
        }

        for (ND_int i = 0; i < lattice->nsym; ++i)
        {
            transpose3x3f(sym_temp + 9 * i, lattice->syms[i].Rmat);

            memcpy(lattice->syms[i].tau, tau + 3 * i, sizeof(ELPH_float) * 3);

            if (i >= (lattice->nsym / (lattice->timerev + 1)))
            {
                lattice->syms[i].time_rev = true;
            }
            else
            {
                lattice->syms[i].time_rev = false;
            }
        }

        free(kiBZtmp);
        free(sym_temp);
    }

    Bcast_symmetries(lattice->nsym, lattice->syms, 0, Comm->commW);
    mpi_error = MPI_Bcast(lattice->kpt_iredBZ, 3 * nibz, ELPH_MPI_float, 0,
                          Comm->commW);
    MPI_error_msg(mpi_error);

    mpi_error = MPI_Bcast(lattice->alat_vec, 9, ELPH_MPI_float, 0, Comm->commW);
    MPI_error_msg(mpi_error);

    reciprocal_vecs(lattice->alat_vec, lattice->blat_vec);
    lattice->volume = fabs(det3x3(lattice->alat_vec));

    lattice->kpt_fullBZ_crys = calloc(3 * nkBZ, sizeof(ELPH_float));
    CHECK_ALLOC(lattice->kpt_fullBZ_crys);

    lattice->kpt_fullBZ = calloc(3 * nkBZ, sizeof(ELPH_float));
    CHECK_ALLOC(lattice->kpt_fullBZ);

    lattice->kmap = malloc(sizeof(int) * nkBZ * 2);
    CHECK_ALLOC(lattice->kmap);

    ND_int tmp_nkBZ = bz_expand(nibz, lattice->nsym, lattice->kpt_iredBZ,
                                lattice->syms, lattice->alat_vec,
                                lattice->kpt_fullBZ_crys, NULL, lattice->kmap);
    if (tmp_nkBZ != nkBZ)
    {
        error_msg("K-point expansion over full BZ failed.");
    }

    matmul_float('N', 'T', lattice->kpt_fullBZ_crys, lattice->blat_vec,
                 lattice->kpt_fullBZ, 1.0f / (2.0f * ELPH_PI), 0.0, 3, 3, 3,
                 nkBZ, 3, 3);

    ELPH_float ntype;
    if (Comm->commW_rank == 0)
    {
        quick_read(dbid, "number_of_atom_species", &ntype);
    }

    mpi_error = MPI_Bcast(&ntype, 1, ELPH_MPI_float, 0, Comm->commW);
    MPI_error_msg(mpi_error);

    pseudo->ntype = rint(ntype);

    char* atom_symbols = NULL;

    if (Comm->commW_rank == 0)
    {
        ELPH_float* natom_per_type =
            malloc(sizeof(ELPH_float) * 2 * pseudo->ntype);
        CHECK_ALLOC(natom_per_type);

        ELPH_float* atomic_numbers = natom_per_type + pseudo->ntype;

        quick_read(dbid, "N_ATOMS", natom_per_type);
        quick_read(dbid, "atomic_numbers", atomic_numbers);

        lattice->natom = 0;
        for (ND_int ia = 0; ia < pseudo->ntype; ++ia)
        {
            lattice->natom += (int)rint(natom_per_type[ia]);
        }
        ND_int nspec_max = rint(find_maxfloat(natom_per_type, pseudo->ntype));

        ELPH_float* atomic_map =
            malloc(sizeof(ELPH_float) * pseudo->ntype * nspec_max);
        CHECK_ALLOC(atomic_map);

        ELPH_float* atom_pos_temp =
            malloc(sizeof(ELPH_float) * pseudo->ntype * nspec_max * 3);
        CHECK_ALLOC(atom_pos_temp);

        quick_read(dbid, "ATOM_MAP", atomic_map);
        quick_read(dbid, "ATOM_POS", atom_pos_temp);

        lattice->atom_type = malloc(sizeof(int) * lattice->natom);
        CHECK_ALLOC(lattice->atom_type);

        atom_symbols = malloc(3 * pseudo->ntype);
        CHECK_ALLOC(atom_symbols);

        lattice->atomic_pos = malloc(sizeof(ELPH_float) * 3 * lattice->natom);
        CHECK_ALLOC(lattice->atomic_pos);

        for (ND_int it = 0; it < pseudo->ntype; ++it)
        {
            ND_int ia_no = rint(atomic_numbers[it]);
            memcpy(atom_symbols + 3 * it, elements + 3 * ia_no,
                   3 * sizeof(char));

            ND_int nspec = rint(natom_per_type[it]);
            for (ND_int ispec = 0; ispec < nspec; ++ispec)
            {
                ND_int iatom = rint(atomic_map[ispec + it * nspec_max] - 1);
                ELPH_float* get_ptr =
                    atom_pos_temp + 3 * (ispec + it * nspec_max);
                ELPH_float* set_ptr = lattice->atomic_pos + 3 * iatom;
                memcpy(set_ptr, get_ptr, 3 * sizeof(ELPH_float));
                lattice->atom_type[iatom] = it;
            }
        }
        free(atomic_map);
        free(atom_pos_temp);
        free(natom_per_type);
    }

    mpi_error = MPI_Bcast(&lattice->natom, 1, MPI_INT, 0, Comm->commW);
    MPI_error_msg(mpi_error);

    lattice->nmodes = 3 * lattice->natom;

    if (Comm->commW_rank != 0)
    {
        lattice->atom_type = malloc(sizeof(int) * lattice->natom);
        CHECK_ALLOC(lattice->atom_type);

        lattice->atomic_pos = malloc(sizeof(ELPH_float) * 3 * lattice->natom);
        CHECK_ALLOC(lattice->atomic_pos);
    }

    mpi_error =
        MPI_Bcast(lattice->atom_type, lattice->natom, MPI_INT, 0, Comm->commW);
    MPI_error_msg(mpi_error);

    mpi_error = MPI_Bcast(lattice->atomic_pos, 3 * lattice->natom,
                          ELPH_MPI_float, 0, Comm->commW);
    MPI_error_msg(mpi_error);

    ELPH_float* nGmax = malloc(sizeof(ELPH_float) * nibz);
    CHECK_ALLOC(nGmax);

    *wfcs = malloc(sizeof(struct WFC) * nibz);
    CHECK_ALLOC(*wfcs);

    struct WFC* wfc_temp = *wfcs;

    if (Comm->commW_rank == 0)
    {
        quick_read(dbid, "WFC_NG", nGmax);
    }

    mpi_error = MPI_Bcast(nGmax, nibz, ELPH_MPI_float, 0, Comm->commW);
    MPI_error_msg(mpi_error);

    ND_int ng_total = rint(dimensions[7]);
    ND_int ng_shell = rint(dimensions[8]);
    ELPH_float* totalGvecs = malloc(sizeof(ELPH_float) * 3 * ng_total);
    CHECK_ALLOC(totalGvecs);

    ELPH_float* Gvecidxs = malloc(sizeof(ELPH_float) * nibz * ng_shell);
    CHECK_ALLOC(Gvecidxs);

    if (Comm->commW_rank == 0)
    {
        quick_read(dbid, "G-VECTORS", totalGvecs);
        quick_read(dbid, "WFC_GRID", Gvecidxs);
        if ((retval = nc_close(dbid)))
        {
            ERR(retval);
        }
    }

    mpi_error =
        MPI_Bcast(totalGvecs, 3 * ng_total, ELPH_MPI_float, 0, Comm->commW);
    MPI_error_msg(mpi_error);

    mpi_error =
        MPI_Bcast(Gvecidxs, nibz * ng_shell, ELPH_MPI_float, 0, Comm->commW);
    MPI_error_msg(mpi_error);

    if (Comm->commW_rank == 0)
    {
        cwk_path_join(SAVEdir, pp_head, temp_str, temp_str_len);
        if ((retval = nc_open(temp_str, NC_NOWRITE, &ppid)))
        {
            ERR(retval);
        }

        {
            ELPH_float kb_pars[4];
            quick_read(ppid, "PARS", kb_pars);
            pseudo->nltimesj = rint(kb_pars[2]);
            pseudo->lmax = rint(kb_pars[3]);
        }
    }

    mpi_error =
        MPI_Bcast(&(pseudo->nltimesj), 1, ELPH_MPI_ND_INT, 0, Comm->commW);
    MPI_error_msg(mpi_error);

    mpi_error = MPI_Bcast(&(pseudo->lmax), 1, MPI_INT, 0, Comm->commW);
    MPI_error_msg(mpi_error);

    pseudo->PP_table =
        malloc(sizeof(ELPH_float) * pseudo->ntype * pseudo->nltimesj * 3);
    CHECK_ALLOC(pseudo->PP_table);

    pseudo->Fsign =
        malloc(sizeof(ELPH_float) * pseudo->ntype * pseudo->nltimesj);
    CHECK_ALLOC(pseudo->Fsign);

    if (Comm->commW_rank == 0)
    {
        quick_read(ppid, "PP_TABLE", pseudo->PP_table);
        quick_read(ppid, "PP_KBS", pseudo->Fsign);

        if ((retval = nc_close(ppid)))
        {
            ERR(retval);
        }
    }

    mpi_error =
        MPI_Bcast(pseudo->PP_table, pseudo->nltimesj * pseudo->ntype * 3,
                  ELPH_MPI_float, 0, Comm->commW);
    MPI_error_msg(mpi_error);

    mpi_error = MPI_Bcast(pseudo->Fsign, pseudo->nltimesj * pseudo->ntype,
                          ELPH_MPI_float, 0, Comm->commW);
    MPI_error_msg(mpi_error);

    ND_int total_kpools = Comm->nkpools * Comm->nqpools;
    ND_int q_kpool_read = nibz / total_kpools;
    ND_int r_kpool_read = nibz % total_kpools;

    for (ND_int ik = 0; ik < nibz; ++ik)
    {
        (wfc_temp + ik)->npw_total = rint(nGmax[ik]);

        ND_int G_shift, pw_this_cpu;
        pw_this_cpu = get_mpi_local_size_idx((wfc_temp + ik)->npw_total,
                                             &G_shift, Comm->commK);

        (wfc_temp + ik)->npw_loc = pw_this_cpu;
        (wfc_temp + ik)->gvec = malloc(sizeof(ELPH_float) * 3 * pw_this_cpu);
        CHECK_ALLOC((wfc_temp + ik)->gvec);

        alloc_and_set_Gvec((wfc_temp + ik)->gvec, ik, totalGvecs, ng_total,
                           Gvecidxs, ng_shell, lat_param, pw_this_cpu, G_shift);

        ND_int spin_stride_len =
            lattice->nbnds * lattice->nspinor * pw_this_cpu;
        (wfc_temp + ik)->wfc =
            malloc(sizeof(ELPH_cmplx) * lattice->nspin * spin_stride_len);
        CHECK_ALLOC((wfc_temp + ik)->wfc);

        ND_int ipool_iibz = get_wf_io_pool(ik, q_kpool_read, r_kpool_read);
        if (Comm->commR_rank == ipool_iibz)
        {
            get_wfc_from_save(spin_stride_len, ik, nibz, lattice->nspin,
                              lattice->nspinor, lattice->start_band,
                              lattice->nbnds, pw_this_cpu, G_shift, SAVEdir,
                              temp_str, temp_str_len, (wfc_temp + ik)->wfc,
                              Comm->commK);
        }

        (wfc_temp + ik)->Fk = malloc(sizeof(ELPH_float) * pseudo->nltimesj *
                                     pseudo->ntype * pw_this_cpu);
        CHECK_ALLOC((wfc_temp + ik)->Fk);

        char small_buf[64];

        snprintf(small_buf, 64, "%s_fragment_%d", pp_head, (int)(ik + 1));
        cwk_path_join(SAVEdir, small_buf, temp_str, temp_str_len);

        if (Comm->commR_rank == ipool_iibz)
        {
            if ((retval = nc_open_par(temp_str, NC_NOWRITE, Comm->commK,
                                      MPI_INFO_NULL, &ppid)))
            {
                ERR(retval);
            }

            snprintf(temp_str, temp_str_len, "PP_KB_K%d", (int)(ik + 1));

            size_t startppkb[] = {0, 0, G_shift};
            size_t countppkb[] = {pseudo->nltimesj, pseudo->ntype, pw_this_cpu};
            quick_read_sub(ppid, temp_str, startppkb, countppkb,
                           (wfc_temp + ik)->Fk);

            if ((retval = nc_close(ppid)))
            {
                ERR(retval);
            }
        }
    }

    for (ND_int ik = 0; ik < nibz; ++ik)
    {
        ND_int ipool_iibz = get_wf_io_pool(ik, q_kpool_read, r_kpool_read);

        ND_int pw_this_cpu = (wfc_temp + ik)->npw_loc;
        ND_int spin_stride_len =
            lattice->nbnds * lattice->nspinor * pw_this_cpu;

        mpi_error =
            MPI_Bcast((wfc_temp + ik)->wfc, lattice->nspin * spin_stride_len,
                      ELPH_MPI_cmplx, ipool_iibz, Comm->commR);
        MPI_error_msg(mpi_error);

        mpi_error = MPI_Bcast((wfc_temp + ik)->Fk,
                              pseudo->nltimesj * pseudo->ntype * pw_this_cpu,
                              ELPH_MPI_float, ipool_iibz, Comm->commR);
        MPI_error_msg(mpi_error);
    }

    free(totalGvecs);
    free(Gvecidxs);

    alloc_and_Compute_f_Coeff(lattice, pseudo);

    ND_int* pseudo_order = malloc(sizeof(ND_int) * pseudo->ntype);
    CHECK_ALLOC(pseudo_order);

    for (ND_int ipot1 = 0; ipot1 < pseudo->ntype; ++ipot1)
    {
        pseudo_order[ipot1] = -1;
    }

    if (Comm->commW_rank == 0)
    {
        for (ND_int ipot1 = 0; ipot1 < pseudo->ntype; ++ipot1)
        {
            char temp_ele[3];

            cwk_path_join(ph_save_dir, pseudo_pots[ipot1], temp_str,
                          temp_str_len);

            get_upf_element(temp_str, temp_ele, NULL);
            bool found = false;

            for (ND_int ipot2 = 0; ipot2 < pseudo->ntype; ++ipot2)
            {
                if (strcmp(temp_ele, atom_symbols + 3 * ipot2) == 0 &&
                    !check_ele_in_array(pseudo_order, pseudo->ntype, ipot2))
                {
                    found = true;
                    pseudo_order[ipot1] = ipot2;
                    break;
                }
            }
            if (!found)
            {
                fprintf(stderr, "Pseudo for element %s not found \n", temp_ele);
                error_msg("Missing pseudopotential");
            }
        }
    }

    mpi_error =
        MPI_Bcast(pseudo_order, pseudo->ntype, ELPH_MPI_ND_INT, 0, Comm->commW);
    MPI_error_msg(mpi_error);

    pseudo->loc_pseudo = malloc(pseudo->ntype * sizeof(struct local_pseudo));
    CHECK_ALLOC(pseudo->loc_pseudo);

    if (Comm->commW_rank == 0)
    {
        for (ND_int ipot = 0; ipot < pseudo->ntype; ++ipot)
        {
            ND_int iorder = pseudo_order[ipot];

            cwk_path_join(ph_save_dir, pseudo_pots[ipot], temp_str,
                          temp_str_len);

            parse_upf(temp_str, pseudo->loc_pseudo + iorder);
        }
    }

    for (ND_int itype = 0; itype < pseudo->ntype; ++itype)
    {
        Bcast_local_pseudo(pseudo->loc_pseudo + itype, true, 0, Comm->commW);
    }

    for (ND_int ipot = 0; ipot < pseudo->ntype; ++ipot)
    {
        pseudo_order[ipot] = pseudo->loc_pseudo[ipot].ngrid;
    }

    pseudo->ngrid_max = find_maxint(pseudo_order, pseudo->ntype);
    lattice->npw_max = find_maxfloat(nGmax, nibz);

    ELPH_float qmax_val = fabs(phonon->qpts_iBZ[0]);
    for (ND_int imax = 0; imax < (phonon->nq_iBZ * 3); ++imax)
    {
        if (fabs(phonon->qpts_iBZ[imax]) > qmax_val)
        {
            qmax_val = fabs(phonon->qpts_iBZ[imax]);
        }
    }

    pseudo->vloc_table->qmax_abs = ceil(fabs(qmax_val)) + 1;

    create_vlocg_table(lattice, pseudo, Comm);

    if (Comm->commW_rank == 0)
    {
        if (pseudo_pots != NULL)
        {
            for (ND_int ipot = 0; ipot < pseudo->ntype; ++ipot)
            {
                free(pseudo_pots[ipot]);
            }
            free(pseudo_pots);
        }
    }

    free(pseudo_order);
    free(atom_symbols);
    free(elements);
    free(nGmax);
    free(temp_str);
    ELPH_stop_clock("Save I.O");
}

/**
 * @brief Free phonon data structure
 *
 * Deallocates all memory associated with phonon structure including
 * q-points, symmetries, and related arrays.
 *
 * @param[in,out] phonon Phonon structure to free
 */
void free_phonon_data(struct Phonon* phonon)
{
    free(phonon->qpts_iBZ);
    free(phonon->qpts_BZ);
    free(phonon->ph_syms);
    free(phonon->qmap);
    free(phonon->nqstar);
    free(phonon->epsilon);
    free(phonon->Zborn);
    free(phonon->Qpole);
}

/**
 * @brief Free all SAVE data structures
 *
 * Deallocates all memory associated with wavefunctions, lattice,
 * pseudopotentials, and phonon data read from SAVE directory.
 *
 * @param[in,out] wfcs Wavefunction array to free
 * @param[in,out] lattice Lattice structure to free
 * @param[in,out] pseudo Pseudopotential structure to free
 * @param[in,out] phonon Phonon structure to free
 */
void free_save_data(struct WFC* wfcs, struct Lattice* lattice,
                    struct Pseudo* pseudo, struct Phonon* phonon)
{
    free_f_Coeff(lattice, pseudo);

    free_vlocg_table(pseudo->vloc_table);

    free(pseudo->Fsign);
    free(pseudo->PP_table);

    if (pseudo->loc_pseudo)
    {
        for (ND_int itype = 0; itype < pseudo->ntype; ++itype)
        {
            free(pseudo->loc_pseudo[itype].Vloc_atomic);
            free(pseudo->loc_pseudo[itype].r_grid);
            free(pseudo->loc_pseudo[itype].rab_grid);
        }
    }
    free(pseudo->loc_pseudo);

    ND_int nkiBZ = lattice->nkpts_iBZ;

    if (wfcs)
    {
        for (ND_int ik = 0; ik < nkiBZ; ++ik)
        {
            free((wfcs + ik)->wfc);
            free((wfcs + ik)->gvec);
            free((wfcs + ik)->Fk);
        }
    }
    free(wfcs);

    free(lattice->atomic_pos);
    free(lattice->atom_type);
    free(lattice->kpt_iredBZ);
    free(lattice->kpt_fullBZ);
    free(lattice->kpt_fullBZ_crys);
    free(lattice->kmap);
    free(lattice->syms);

    free_phonon_data(phonon);
}

/**
 * @brief Allocate and set G-vectors for a specific k-point
 *
 * Extracts G-vectors for a given k-point from the global G-vector table
 * and converts them to Cartesian coordinates normalized by lattice parameters.
 *
 * @param[out] gvec Output G-vectors in Cartesian coordinates, shape: (nG,3)
 * @param[in] ik K-point index
 * @param[in] totalGvecs Global G-vector table, shape: (3,ng_total)
 * @param[in] ng_total Total number of G-vectors in global table
 * @param[in] Gvecidxs Indices mapping k-points to G-vectors, shape:
 * (nkiBZ,ng_shell)
 * @param[in] ng_shell Maximum number of G-vectors per k-point
 * @param[in] lat_param Lattice parameters [a, b, c]
 * @param[in] nG Number of local G-vectors for this k-point
 * @param[in] nG_shift Starting index for local G-vectors (MPI offset)
 *
 * @note G-vector indices in Gvecidxs use Fortran convention (1-based)
 * @note Output gvec is normalized by lattice parameters for each direction
 */
static void alloc_and_set_Gvec(
    ELPH_float* gvec, const ND_int ik, const ELPH_float* totalGvecs,
    const ND_int ng_total, const ELPH_float* Gvecidxs, const ND_int ng_shell,
    const ELPH_float* lat_param, const ND_int nG, const ND_int nG_shift)
{
    const ELPH_float* gidx_temp = Gvecidxs + ik * ng_shell;

    for (ND_int ig = 0; ig < nG; ++ig)
    {
        ND_int gidx = rint(gidx_temp[ig + nG_shift] - 1);
        if (gidx < 0)
        {
            error_msg("Wrong g indices");
        }
        ELPH_float* gvec_temp = gvec + ig * 3;

        for (int i = 0; i < 3; ++i)
        {
            gvec_temp[i] = totalGvecs[ng_total * i + gidx] / lat_param[i];
        }
    }
}

/**
 * @brief Read wavefunctions from SAVE directory for specific k-point
 *
 * Reads wavefunction data using parallel NetCDF for a given k-point and
 * all spin channels. Each MPI rank reads its local portion of G-vectors.
 *
 * @param[in] spin_stride_len Size of single spin channel: nbnds * nspinor * nG
 * @param[in] ik K-point index (0-based)
 * @param[in] nkiBZ Total number of k-points in iBZ
 * @param[in] nspin Number of spin channels (1 or 2)
 * @param[in] nspinor Number of spinor components (1 or 2)
 * @param[in] start_band Starting band index (Fortran: 1-based)
 * @param[in] nbnds Number of bands to read
 * @param[in] nG Number of local G-vectors
 * @param[in] G_shift Starting G-vector index for this MPI rank
 * @param[in] save_dir Path to SAVE directory
 * @param[in,out] work_array Preallocated string buffer for path construction
 * @param[in] work_array_len Length of work_array buffer
 * @param[out] out_wfc Output wavefunctions, shape: (nspin, nbnds, nspinor, nG)
 * @param[in] comm MPI communicator for parallel I/O
 *
 * @note Not thread-safe - should not be called with OpenMP
 * @note Uses collective parallel NetCDF I/O
 */
static void get_wfc_from_save(ND_int spin_stride_len, ND_int ik, ND_int nkiBZ,
                              ND_int nspin, ND_int nspinor, ND_int start_band,
                              ND_int nbnds, ND_int nG, ND_int G_shift,
                              const char* save_dir, char* work_array,
                              const size_t work_array_len, ELPH_cmplx* out_wfc,
                              MPI_Comm comm)
{
    int wfID, retval;

    for (ND_int is = 0; is < nspin; ++is)
    {
        char tmp_buf[64];
        snprintf(tmp_buf, sizeof(tmp_buf), "ns.wf_fragments_%d_1",
                 (int)(is * nkiBZ + (ik + 1)));
        cwk_path_join(save_dir, tmp_buf, work_array, work_array_len);

        if ((retval = nc_open_par(work_array, NC_NOWRITE, comm, MPI_INFO_NULL,
                                  &wfID)))
        {
            ERR(retval);
        }

        snprintf(work_array, work_array_len,
                 "WF_COMPONENTS_@_SP_POL%d_K%d_BAND_GRP_1", (int)(is + 1),
                 (int)(ik + 1));

        size_t startp[4] = {start_band - 1, 0, G_shift, 0};
        size_t countp[4] = {nbnds, nspinor, nG, 2};
        quick_read_sub(wfID, work_array, startp, countp,
                       out_wfc + is * spin_stride_len);

        if ((retval = nc_close(wfID)))
        {
            ERR(retval);
        }
    }
}

/**
 * @brief Serial NetCDF read of entire variable
 *
 * Reads complete variable data from NetCDF file into output buffer.
 *
 * @param[in] ncid NetCDF file ID
 * @param[in] var_name Variable name to read
 * @param[out] data_out Output buffer (must be preallocated)
 *
 * @note Serial operation - not for parallel I/O
 */
static void quick_read(const int ncid, char* var_name, void* data_out)
{
    int varid, retval;

    if ((retval = nc_inq_varid(ncid, var_name, &varid)))
    {
        ERR(retval);
    }

    if ((retval = nc_get_var(ncid, varid, data_out)))
    {
        ERR(retval);
    }
}

/**
 * @brief Parallel NetCDF read of variable slice
 *
 * Reads a hyperslab (slice) of variable data from NetCDF file using
 * collective parallel I/O.
 *
 * @param[in] ncid NetCDF file ID (opened with nc_open_par)
 * @param[in] var_name Variable name to read
 * @param[in] startp Starting indices for each dimension
 * @param[in] countp Count of elements to read in each dimension
 * @param[out] data_out Output buffer (must be preallocated)
 *
 * @note Uses collective parallel I/O (NC_COLLECTIVE)
 * @note File must be opened with nc_open_par for parallel access
 */
static void quick_read_sub(const int ncid, char* var_name, const size_t* startp,
                           const size_t* countp, void* data_out)
{
    int varid, retval;

    if ((retval = nc_inq_varid(ncid, var_name, &varid)))
    {
        ERR(retval);
    }

    if ((retval = nc_var_par_access(ncid, varid, NC_COLLECTIVE)))
    {
        ERR(retval);
    }

    if ((retval = nc_get_vara(ncid, varid, startp, countp, data_out)))
    {
        ERR(retval);
    }
}

/**
 * @brief Check if element exists in array
 *
 * Searches for a specific element value in an integer array.
 *
 * @param[in] arr_in Array to search
 * @param[in] nelements Number of elements in array
 * @param[in] check_ele Element value to find
 * @return true if element found, false otherwise
 */
static bool check_ele_in_array(ND_int* arr_in, ND_int nelements,
                               ND_int check_ele)
{
    bool found = false;
    for (int ii = 0; ii < nelements; ++ii)
    {
        if (arr_in[ii] == check_ele)
        {
            found = true;
            break;
        }
    }
    return found;
}

/**
 * @brief Determine which MPI pool should read wavefunction data
 *
 * Computes the MPI rank (pool) responsible for reading wavefunction data
 * for a given k-point index. Distribution uses a load-balancing scheme
 * where some pools read (q+1) k-points and others read q k-points.
 *
 * Distribution scheme:
 * - First r pools read (q+1) k-points each
 * - Remaining pools read q k-points each
 *
 * where nibz = total_pools * q + r
 *
 * @param[in] ik K-point index (0-based)
 * @param[in] q Base number of k-points per pool (nibz / total_pools)
 * @param[in] r Remainder k-points (nibz % total_pools)
 * @return MPI rank (pool ID) that should read this k-point
 *
 * @note Assumes ik is in valid range [0, nibz)
 */
static inline ND_int get_wf_io_pool(ND_int ik, ND_int q, ND_int r)
{
    ND_int offset = r * (q + 1);
    if (ik < offset)
    {
        return ik / (q + 1);
    }
    else
    {
        return (ik - r) / q;
    }
}
